# CAT Model Training Guide
# H∆∞·ªõng D·∫´n Training M√¥ H√¨nh CAT (Computerized Adaptive Testing)

## üìö Table of Contents

- [Gi·ªõi thi·ªáu / Introduction](#gi·ªõi-thi·ªáu--introduction)
- [T·∫°i sao c·∫ßn Training CAT?](#t·∫°i-sao-c·∫ßn-training-cat)
- [C√°ch th·ª©c ho·∫°t ƒë·ªông](#c√°ch-th·ª©c-ho·∫°t-ƒë·ªông)
- [Method 1: Manual Calibration (Quick Start)](#method-1-manual-calibration-quick-start)
- [Method 2: Data-Based Calibration](#method-2-data-based-calibration)
- [Method 3: IRT-Based Calibration (Advanced)](#method-3-irt-based-calibration-advanced)
- [Best Practices](#best-practices)

## Gi·ªõi thi·ªáu / Introduction

CAT (Computerized Adaptive Testing) l√† h·ªá th·ªëng thi th√≠ch ·ª©ng, t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh ƒë·ªô kh√≥ c√¢u h·ªèi d·ª±a tr√™n kh·∫£ nƒÉng c·ªßa h·ªçc sinh.

**L∆∞u √Ω quan tr·ªçng**: B·∫°n **KH√îNG B·∫ÆT BU·ªòC** ph·∫£i training CAT model. H·ªá th·ªëng ƒë√£ s·∫µn s√†ng ho·∫°t ƒë·ªông ngay khi b·∫°n g√°n ƒë·ªô kh√≥ cho c√¢u h·ªèi.

## T·∫°i sao c·∫ßn Training CAT?

### ‚ùå Kh√¥ng c·∫ßn training trong c√°c tr∆∞·ªùng h·ª£p:
- M·ªõi b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng h·ªá th·ªëng
- S·ªë l∆∞·ª£ng h·ªçc sinh < 50
- Ch∆∞a c√≥ ƒë·ªß d·ªØ li·ªáu t·ª´ c√°c k·ª≥ thi
- Ch·ªâ d√πng ƒë·ªÉ th·ª≠ nghi·ªám

### ‚úÖ N√™n training khi:
- ƒê√£ c√≥ 100+ h·ªçc sinh l√†m b√†i
- Mu·ªën c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c
- C√≥ d·ªØ li·ªáu t·ª´ 3-6 th√°ng s·ª≠ d·ª•ng
- Ph√°t hi·ªán ƒë·ªô kh√≥ kh√¥ng ch√≠nh x√°c

## C√°ch th·ª©c ho·∫°t ƒë·ªông

### IRT (Item Response Theory) 1PL Model

CAT s·ª≠ d·ª•ng m√¥ h√¨nh IRT 1-parameter:

```
P(Œ∏, b) = 1 / (1 + e^(-(Œ∏ - b)))
```

Trong ƒë√≥:
- `Œ∏` (theta): Ability c·ªßa h·ªçc sinh (-3 ƒë·∫øn +3)
- `b`: Difficulty c·ªßa c√¢u h·ªèi (-3 ƒë·∫øn +3)
- `P`: X√°c su·∫•t tr·∫£ l·ªùi ƒë√∫ng

### Quy tr√¨nh CAT

1. **Kh·ªüi t·∫°o**: Ability ban ƒë·∫ßu = 0 (trung b√¨nh)
2. **Ch·ªçn c√¢u h·ªèi**: Ch·ªçn c√¢u c√≥ ƒë·ªô kh√≥ g·∫ßn v·ªõi ability hi·ªán t·∫°i
3. **H·ªçc sinh tr·∫£ l·ªùi**: ƒê√∫ng ‚Üí tƒÉng ability, Sai ‚Üí gi·∫£m ability
4. **C·∫≠p nh·∫≠t ability**: D√πng Maximum Likelihood Estimation (MLE)
5. **L·∫∑p l·∫°i** cho ƒë·∫øn khi ƒë·∫°t ƒë·ªô ch√≠nh x√°c ho·∫∑c h·∫øt c√¢u h·ªèi

## Method 1: Manual Calibration (Quick Start)

**Th·ªùi gian**: 5 ph√∫t  
**Y√™u c·∫ßu**: Hi·ªÉu bi·∫øt v·ªÅ m√¥n h·ªçc  
**ƒê·ªô ch√≠nh x√°c**: 70-80%

### B∆∞·ªõc 1: X√°c ƒë·ªãnh ƒë·ªô kh√≥

Khi t·∫°o c√¢u h·ªèi, g√°n ƒë·ªô kh√≥ theo quy t·∫Øc:

```
ƒê·ªô kh√≥ 0.0-1.0:

Easy (0.0 - 0.3):
- Ki·∫øn th·ª©c c∆° b·∫£n, ghi nh·ªõ
- C√¥ng th·ª©c ƒë∆°n gi·∫£n
- H·ªçc sinh y·∫øu c√≥ th·ªÉ l√†m ƒë∆∞·ª£c
V√≠ d·ª•: "2 + 2 = ?"

Medium (0.3 - 0.7):
- V·∫≠n d·ª•ng ki·∫øn th·ª©c
- T√≠nh to√°n trung b√¨nh
- ƒêa s·ªë h·ªçc sinh l√†m ƒë∆∞·ª£c
V√≠ d·ª•: "T√≠nh ƒë·∫°o h√†m c·ªßa x^2"

Hard (0.7 - 1.0):
- Ph√¢n t√≠ch, t·ªïng h·ª£p
- Nhi·ªÅu b∆∞·ªõc gi·∫£i
- Ch·ªâ h·ªçc sinh gi·ªèi l√†m ƒë∆∞·ª£c
V√≠ d·ª•: "Ch·ª©ng minh ƒë·ªãnh l√Ω..."
```

### B∆∞·ªõc 2: T·∫°o c√¢u h·ªèi v·ªõi API

```bash
curl -X POST http://localhost:3000/api/questions \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "type": "multiple-choice",
    "question_text": "T√≠nh 2 + 2 = ?",
    "options": ["3", "4", "5", "6"],
    "correct_answer": "1",
    "difficulty": 0.2,
    "topic": "To√°n h·ªçc c∆° b·∫£n"
  }'
```

### B∆∞·ªõc 3: Ki·ªÉm tra

Sau khi c√≥ 10-20 h·ªçc sinh l√†m b√†i, xem analytics:
- N·∫øu c√¢u "d·ªÖ" m√† t·ª∑ l·ªá ƒë√∫ng < 60% ‚Üí TƒÉng ƒë·ªô kh√≥
- N·∫øu c√¢u "kh√≥" m√† t·ª∑ l·ªá ƒë√∫ng > 80% ‚Üí Gi·∫£m ƒë·ªô kh√≥

## Method 2: Data-Based Calibration

**Th·ªùi gian**: 30 ph√∫t  
**Y√™u c·∫ßu**: Python, c√≥ d·ªØ li·ªáu t·ª´ 50+ h·ªçc sinh  
**ƒê·ªô ch√≠nh x√°c**: 85-90%

### B∆∞·ªõc 1: Export d·ªØ li·ªáu t·ª´ Supabase

Trong Supabase SQL Editor:

```sql
-- Export question performance data
SELECT 
    q.id as question_id,
    q.question_text,
    q.difficulty as current_difficulty,
    COUNT(DISTINCT ea.student_id) as total_attempts,
    SUM(CASE WHEN resp->>'is_correct' = 'true' THEN 1 ELSE 0 END) as correct_count
FROM questions q
LEFT JOIN exam_attempts ea ON ea.responses @> jsonb_build_array(
    jsonb_build_object('question_id', q.id)
)
GROUP BY q.id
HAVING COUNT(DISTINCT ea.student_id) >= 10
ORDER BY q.difficulty;
```

Save k·∫øt qu·∫£ v√†o `question_performance.csv`

### B∆∞·ªõc 2: T√≠nh to√°n ƒë·ªô kh√≥ m·ªõi

Create file `calibrate_cat.py`:

```python
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('question_performance.csv')

# Calculate p-value (proportion correct)
df['p_value'] = df['correct_count'] / df['total_attempts']

# Convert p-value to difficulty (0-1 scale)
# Harder questions have lower p-value, higher difficulty
df['new_difficulty'] = 1 - df['p_value']

# Normalize to 0-1 range
df['new_difficulty'] = (df['new_difficulty'] - df['new_difficulty'].min()) / \
                       (df['new_difficulty'].max() - df['new_difficulty'].min())

# Blend with current difficulty (70% new, 30% old)
df['calibrated_difficulty'] = 0.7 * df['new_difficulty'] + 0.3 * df['current_difficulty']

# Save results
df[['question_id', 'calibrated_difficulty']].to_csv('updated_difficulties.csv', index=False)

print("Calibration complete!")
print(f"Updated {len(df)} questions")
print("\nDifficulty distribution:")
print(df['calibrated_difficulty'].describe())
```

Run script:

```bash
pip install pandas numpy
python calibrate_cat.py
```

### B∆∞·ªõc 3: Update v√†o database

```python
import pandas as pd
import requests

df = pd.read_csv('updated_difficulties.csv')

API_URL = 'http://localhost:3000/api'
TOKEN = 'your_jwt_token'

headers = {
    'Authorization': f'Bearer {TOKEN}',
    'Content-Type': 'application/json'
}

for _, row in df.iterrows():
    question_id = row['question_id']
    difficulty = row['calibrated_difficulty']
    
    response = requests.put(
        f'{API_URL}/questions/{question_id}',
        headers=headers,
        json={'difficulty': float(difficulty)}
    )
    
    if response.status_code == 200:
        print(f'‚úì Updated question {question_id}')
    else:
        print(f'‚úó Failed to update {question_id}')

print("\nAll questions updated!")
```

## Method 3: IRT-Based Calibration (Advanced)

**Th·ªùi gian**: 2-3 gi·ªù  
**Y√™u c·∫ßu**: Python, R, Statistical knowledge, 100+ responses per question  
**ƒê·ªô ch√≠nh x√°c**: 95%+

### Y√™u c·∫ßu

- 100+ h·ªçc sinh ƒë√£ l√†m b√†i
- M·ªói c√¢u h·ªèi c√≥ t·ªëi thi·ªÉu 100 responses
- Ki·∫øn th·ª©c v·ªÅ IRT v√† statistics

### B∆∞·ªõc 1: Prepare Data

```python
import pandas as pd
import numpy as np
from sqlalchemy import create_engine

# Connect to Supabase
engine = create_engine('postgresql://user:pass@host:port/database')

# Get response data
query = """
SELECT 
    ea.student_id,
    ea.exam_id,
    jsonb_array_elements(ea.responses) as response
FROM exam_attempts ea
WHERE ea.status = 'completed'
"""

responses = pd.read_sql(query, engine)

# Parse JSON responses
responses['question_id'] = responses['response'].apply(lambda x: x['question_id'])
responses['is_correct'] = responses['response'].apply(lambda x: x.get('is_correct', False))

# Pivot to wide format (students x questions)
response_matrix = responses.pivot_table(
    index='student_id',
    columns='question_id',
    values='is_correct',
    aggfunc='first'
).fillna(-1)  # -1 for not attempted

# Save for IRT analysis
response_matrix.to_csv('irt_data.csv')
```

### B∆∞·ªõc 2: IRT Analysis with R

Install R package:

```r
install.packages("mirt")
```

Create `irt_analysis.R`:

```r
library(mirt)

# Load data
data <- read.csv('irt_data.csv', row.names=1)

# Remove questions with < 100 responses
valid_cols <- colSums(data != -1) >= 100
data <- data[, valid_cols]

# Fit 1PL model (Rasch model)
model <- mirt(data, model=1, itemtype='Rasch', verbose=TRUE)

# Extract item parameters
params <- coef(model, IRTpars=TRUE, simplify=TRUE)
item_params <- params$items

# Difficulty parameters
difficulties <- item_params[, 'b']

# Normalize to 0-1 scale
difficulties_normalized <- (difficulties - min(difficulties)) / 
                           (max(difficulties) - min(difficulties))

# Save results
results <- data.frame(
    question_id = colnames(data),
    irt_difficulty = difficulties_normalized,
    discrimination = item_params[, 'a'],
    fit_statistics = itemfit(model)$outfit
)

write.csv(results, 'irt_results.csv', row.names=FALSE)

# Model fit statistics
print("Model Fit:")
print(M2(model))

# Plot item characteristic curves
plot(model, type='trace', which.items=1:min(20, ncol(data)))
```

Run in R:

```bash
Rscript irt_analysis.R
```

### B∆∞·ªõc 3: Validate and Update

```python
import pandas as pd

# Load IRT results
irt_results = pd.read_csv('irt_results.csv')

# Filter questions with good fit (outfit < 1.5)
good_fit = irt_results[irt_results['fit_statistics'] < 1.5]

print(f"Questions with good fit: {len(good_fit)}/{len(irt_results)}")

# Update database (same as Method 2, B∆∞·ªõc 3)
# Use good_fit['irt_difficulty'] as new difficulty values
```

## Best Practices

### 1. S·ªë l∆∞·ª£ng c√¢u h·ªèi

```
Recommended question bank size:
- Minimum: 50 questions
- Good: 100-200 questions
- Excellent: 300+ questions

Difficulty distribution:
- Easy (0.0-0.3): 20-30%
- Medium (0.3-0.7): 40-50%
- Hard (0.7-1.0): 20-30%
```

### 2. T·∫ßn su·∫•t calibration

```
First 3 months: Manual calibration
After 100 students: Data-based calibration (monthly)
After 500 students: IRT-based calibration (quarterly)
Ongoing: Monitor and adjust
```

### 3. Quality checks

```python
# Check difficulty distribution
def check_difficulty_distribution(difficulties):
    easy = sum(1 for d in difficulties if d < 0.3)
    medium = sum(1 for d in difficulties if 0.3 <= d < 0.7)
    hard = sum(1 for d in difficulties if d >= 0.7)
    
    total = len(difficulties)
    print(f"Easy: {easy/total*100:.1f}%")
    print(f"Medium: {medium/total*100:.1f}%")
    print(f"Hard: {hard/total*100:.1f}%")
    
    # Ideal distribution
    if 20 <= easy/total*100 <= 35 and \
       40 <= medium/total*100 <= 55:
        print("‚úì Good distribution!")
    else:
        print("‚ö† Adjust distribution")
```

### 4. Monitoring CAT performance

```sql
-- Check CAT effectiveness
SELECT 
    AVG(questions_administered) as avg_questions,
    AVG(standard_error) as avg_precision,
    AVG(ABS(ability_estimate)) as avg_ability
FROM exam_attempts
WHERE cat_state IS NOT NULL
  AND status = 'completed';

-- Ideal values:
-- avg_questions: 10-15 (efficient)
-- avg_precision: < 0.3 (precise)
-- avg_ability: 0.5-1.5 (good spread)
```

## Troubleshooting

### Problem: T·∫•t c·∫£ h·ªçc sinh ƒë·ªÅu ƒëi·ªÉm cao/th·∫•p

**Solution**: C√¢u h·ªèi qu√° d·ªÖ/kh√≥ ho·∫∑c ph√¢n ph·ªëi kh√¥ng ƒë·ªÅu

```python
# Adjust difficulties
df['difficulty'] = df['difficulty'] * 0.8  # Make easier
# or
df['difficulty'] = df['difficulty'] * 1.2  # Make harder
```

### Problem: CAT test qu√° ng·∫Øn/d√†i

**Solution**: ƒêi·ªÅu ch·ªânh CAT settings

```json
{
  "cat_settings": {
    "initial_ability": 0,
    "precision_threshold": 0.25,  // Lower = more questions
    "min_questions": 8,            // Increase minimum
    "max_questions": 25            // Decrease maximum
  }
}
```

### Problem: M·ªôt s·ªë c√¢u h·ªèi kh√¥ng bao gi·ªù ƒë∆∞·ª£c ch·ªçn

**Solution**: ƒê·ªô kh√≥ kh√¥ng ph√π h·ª£p ho·∫∑c qu√° √≠t c√¢u d·ªÖ/kh√≥

```sql
-- Find unused questions
SELECT q.id, q.difficulty, 
       COUNT(DISTINCT ea.id) as times_used
FROM questions q
LEFT JOIN exam_attempts ea ON 
    ea.responses @> jsonb_build_array(
        jsonb_build_object('question_id', q.id)
    )
GROUP BY q.id
HAVING COUNT(DISTINCT ea.id) < 5
ORDER BY q.difficulty;
```

## Resources

- [IRT on Wikipedia](https://en.wikipedia.org/wiki/Item_response_theory)
- [mirt package documentation](https://cran.r-project.org/web/packages/mirt/mirt.pdf)
- [CAT Tutorial](https://www.assess.com/cat-tutorial/)

## Summary

**Quick Start (5 minutes)**:
```bash
# Just assign difficulties manually:
Easy: 0.0-0.3
Medium: 0.3-0.7
Hard: 0.7-1.0

# Start using immediately!
```

**Regular Maintenance (monthly)**:
```bash
# Export data ‚Üí Calculate new difficulties ‚Üí Update database
python calibrate_cat.py
```

**Advanced Optimization (quarterly)**:
```bash
# Full IRT analysis
Rscript irt_analysis.R
```

**Remember**: CAT works immediately with manual calibration. Advanced training ch·ªâ c·∫ßn thi·∫øt khi b·∫°n mu·ªën t·ªëi ∆∞u h√≥a ƒë·ªô ch√≠nh x√°c sau khi ƒë√£ c√≥ nhi·ªÅu d·ªØ li·ªáu.
