# Anti-Cheat Computer Vision Model Training Guide
# H∆∞·ªõng D·∫´n Training M√¥ H√¨nh Ch·ªëng Gian L·∫≠n B·∫±ng Th·ªã Gi√°c M√°y T√≠nh

## üìö Table of Contents

- [Gi·ªõi thi·ªáu](#gi·ªõi-thi·ªáu)
- [T·∫°i sao c·∫ßn Anti-Cheat?](#t·∫°i-sao-c·∫ßn-anti-cheat)
- [H·ªá th·ªëng m·∫∑c ƒë·ªãnh (Kh√¥ng c·∫ßn training)](#h·ªá-th·ªëng-m·∫∑c-ƒë·ªãnh-kh√¥ng-c·∫ßn-training)
- [Khi n√†o c·∫ßn training custom model?](#khi-n√†o-c·∫ßn-training-custom-model)
- [Method 1: S·ª≠ d·ª•ng BlazeFace (Recommended)](#method-1-s·ª≠-d·ª•ng-blazeface-recommended)
- [Method 2: Training Custom Model](#method-2-training-custom-model)
- [Method 3: Fine-tuning Pre-trained Models](#method-3-fine-tuning-pre-trained-models)
- [Deployment v√† Integration](#deployment-v√†-integration)
- [Best Practices](#best-practices)

## Gi·ªõi thi·ªáu

Anti-cheat system s·ª≠ d·ª•ng **computer vision** ƒë·ªÉ ph√°t hi·ªán h√†nh vi gian l·∫≠n qua webcam:

### C√°c h√†nh vi ph√°t hi·ªán ƒë∆∞·ª£c:
1. **Kh√¥ng c√≥ khu√¥n m·∫∑t** (`no_face`) - H·ªçc sinh r·ªùi kh·ªèi m√†n h√¨nh
2. **Nhi·ªÅu khu√¥n m·∫∑t** (`multiple_faces`) - C√≥ ng∆∞·ªùi kh√°c trong ph√≤ng
3. **Nh√¨n ƒëi ch·ªó kh√°c** (`looking_away`) - Kh√¥ng t·∫≠p trung v√†o m√†n h√¨nh
4. **Chuy·ªÉn tab** (`tab_switch`) - M·ªü tab/c·ª≠a s·ªï kh√°c

### Warning levels:
- **Low**: Vi ph·∫°m nh·ªè, l·∫ßn ƒë·∫ßu
- **Medium**: Vi ph·∫°m l·∫∑p l·∫°i
- **High**: Vi ph·∫°m nghi√™m tr·ªçng
- **Auto-flag**: T·ª± ƒë·ªông ƒë√°nh d·∫•u sau 3 warnings high ho·∫∑c 10 warnings t·ªïng

## T·∫°i sao c·∫ßn Anti-Cheat?

‚úÖ **N√™n d√πng khi**:
- Thi ch√≠nh th·ª©c, c√≥ ƒëi·ªÉm s·ªë quan tr·ªçng
- Thi t·ª´ xa (remote/online)
- Lo ng·∫°i gian l·∫≠n
- C·∫ßn gi√°m s√°t nhi·ªÅu h·ªçc sinh c√πng l√∫c

‚ùå **Kh√¥ng c·∫ßn khi**:
- B√†i t·∫≠p v·ªÅ nh√†
- Thi th·ª≠, luy·ªán t·∫≠p
- L·ªõp h·ªçc nh·ªè (< 10 h·ªçc sinh)
- Thi tr·ª±c ti·∫øp t·∫°i tr∆∞·ªùng

## H·ªá th·ªëng m·∫∑c ƒë·ªãnh (Kh√¥ng c·∫ßn training)

### BlazeFace Model (Google)

**Intelligence Test Platform ƒë√£ t√≠ch h·ª£p s·∫µn BlazeFace** - m·ªôt model ph√°t hi·ªán khu√¥n m·∫∑t nh·∫π, nhanh t·ª´ Google.

**∆Øu ƒëi·ªÉm**:
- ‚úÖ Kh√¥ng c·∫ßn training
- ‚úÖ Ch·∫°y tr·ª±c ti·∫øp tr√™n browser (TensorFlow.js)
- ‚úÖ Nhanh (60 FPS)
- ‚úÖ Nh·∫π (~1MB)
- ‚úÖ Ch√≠nh x√°c cao (95%+)

**Nh∆∞·ª£c ƒëi·ªÉm**:
- ‚ùå Ch·ªâ ph√°t hi·ªán khu√¥n m·∫∑t, kh√¥ng ph√¢n lo·∫°i h√†nh vi ph·ª©c t·∫°p
- ‚ùå Kh√¥ng t√πy ch·ªânh ƒë∆∞·ª£c

### S·ª≠ d·ª•ng BlazeFace

Trong client code (`Intelligence-Test` repository):

```typescript
// src/services/antiCheatService.ts
import * as blazeface from '@tensorflow-models/blazeface';
import '@tensorflow/tfjs';

class AntiCheatService {
  private model: blazeface.BlazeFaceModel | null = null;

  async initialize() {
    this.model = await blazeface.load();
    console.log('BlazeFace model loaded');
  }

  async detectFaces(videoElement: HTMLVideoElement) {
    if (!this.model) {
      await this.initialize();
    }

    const predictions = await this.model!.estimateFaces(videoElement, false);
    
    return {
      faceCount: predictions.length,
      faces: predictions.map(pred => ({
        topLeft: pred.topLeft,
        bottomRight: pred.bottomRight,
        probability: pred.probability
      }))
    };
  }

  async analyzeFrame(videoElement: HTMLVideoElement) {
    const { faceCount, faces } = await this.detectFaces(videoElement);

    // Check violations
    const warnings = [];

    if (faceCount === 0) {
      warnings.push({
        type: 'no_face',
        severity: 'medium',
        details: 'No face detected'
      });
    } else if (faceCount > 1) {
      warnings.push({
        type: 'multiple_faces',
        severity: 'high',
        details: `${faceCount} faces detected`
      });
    }

    // Check if looking away (based on face position)
    if (faceCount === 1) {
      const face = faces[0];
      const videoCenter = {
        x: videoElement.videoWidth / 2,
        y: videoElement.videoHeight / 2
      };
      
      const faceCenter = {
        x: (face.topLeft[0] + face.bottomRight[0]) / 2,
        y: (face.topLeft[1] + face.bottomRight[1]) / 2
      };

      const distance = Math.sqrt(
        Math.pow(faceCenter.x - videoCenter.x, 2) +
        Math.pow(faceCenter.y - videoCenter.y, 2)
      );

      if (distance > videoElement.videoWidth * 0.3) {
        warnings.push({
          type: 'looking_away',
          severity: 'low',
          details: 'Face not centered'
        });
      }
    }

    return warnings;
  }
}

export const antiCheatService = new AntiCheatService();
```

### G·ª≠i warnings v·ªÅ server

```typescript
// Submit warning to server
async function submitWarning(attemptId: string, warning: any) {
  await fetch(`http://localhost:3000/api/attempts/${attemptId}/submit-warning`, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${token}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(warning)
  });
}

// Monitor during exam
setInterval(async () => {
  const warnings = await antiCheatService.analyzeFrame(videoElement);
  
  for (const warning of warnings) {
    await submitWarning(attemptId, warning);
  }
}, 5000); // Check every 5 seconds
```

## Khi n√†o c·∫ßn training custom model?

### ‚úÖ Training custom model khi:

1. **Ph√°t hi·ªán h√†nh vi c·ª• th·ªÉ**:
   - Nh√¨n v√†o gi·∫•y nh√°p (specific to your setup)
   - S·ª≠ d·ª•ng ƒëi·ªán tho·∫°i
   - N√≥i chuy·ªán v·ªõi ai ƒë√≥

2. **M√¥i tr∆∞·ªùng ƒë·∫∑c bi·ªát**:
   - Ph√≤ng thi c√≥ setup camera ƒë·∫∑c bi·ªát
   - Y√™u c·∫ßu detection ch√≠nh x√°c h∆°n
   - C·∫ßn ph√¢n lo·∫°i nhi·ªÅu h√†nh vi ph·ª©c t·∫°p

3. **T·ªëi ∆∞u hi·ªáu su·∫•t**:
   - M√°y t√≠nh y·∫øu, c·∫ßn model nh·∫π h∆°n
   - C·∫ßn FPS cao h∆°n
   - Bandwidth th·∫•p

## Method 2: Training Custom Model

### B∆∞·ªõc 1: Thu th·∫≠p Dataset

**Y√™u c·∫ßu**:
- 500-1000 ·∫£nh cho m·ªói class
- Webcam t∆∞∆°ng t·ª± v·ªõi h·ªçc sinh s·∫Ω d√πng
- ƒêi·ªÅu ki·ªán √°nh s√°ng ƒëa d·∫°ng

#### Setup thu th·∫≠p

```python
import cv2
import os
from datetime import datetime

# Create directories
os.makedirs('dataset/normal', exist_ok=True)
os.makedirs('dataset/suspicious', exist_ok=True)

cap = cv2.VideoCapture(0)
mode = 'normal'  # or 'suspicious'
count = 0

print(f"Collecting images for '{mode}' class")
print("Press SPACE to capture, Q to quit, M to switch mode")

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    cv2.putText(frame, f"Mode: {mode} | Count: {count}", 
                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow('Data Collection', frame)
    
    key = cv2.waitKey(1) & 0xFF
    
    if key == ord(' '):  # Space to capture
        filename = f"{mode}_{count}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
        cv2.imwrite(f'dataset/{mode}/{filename}', frame)
        count += 1
        print(f"Saved: {filename}")
    
    elif key == ord('m'):  # Switch mode
        mode = 'suspicious' if mode == 'normal' else 'normal'
        count = 0
        print(f"Switched to '{mode}' mode")
    
    elif key == ord('q'):  # Quit
        break

cap.release()
cv2.destroyAllWindows()

print(f"\nTotal images collected:")
print(f"Normal: {len(os.listdir('dataset/normal'))}")
print(f"Suspicious: {len(os.listdir('dataset/suspicious'))}")
```

#### H√†nh vi c·∫ßn thu th·∫≠p

**Normal behavior** (500+ ·∫£nh):
- Nh√¨n th·∫≥ng v√†o m√†n h√¨nh
- 1 khu√¥n m·∫∑t duy nh·∫•t
- T·∫≠p trung
- ƒêa d·∫°ng g√≥c nh√¨n nh·∫π (¬±15¬∞)

**Suspicious behavior** (500+ ·∫£nh):
- Nh√¨n ƒëi ch·ªó kh√°c (tr√°i, ph·∫£i, l√™n, xu·ªëng)
- Nhi·ªÅu ng∆∞·ªùi trong khung h√¨nh
- Kh√¥ng c√≥ khu√¥n m·∫∑t
- D√πng ƒëi·ªán tho·∫°i
- Nh√¨n v√†o gi·∫•y nh√°p

### B∆∞·ªõc 2: Preprocessing

```python
import cv2
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Data augmentation
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2
)

# Load training data
train_generator = datagen.flow_from_directory(
    'dataset/',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

# Load validation data
validation_generator = datagen.flow_from_directory(
    'dataset/',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='validation'
)

print(f"Found {train_generator.n} training images")
print(f"Found {validation_generator.n} validation images")
print(f"Classes: {train_generator.class_indices}")
```

### B∆∞·ªõc 3: Train Model

#### Option A: Simple CNN (Lightweight)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Build model
model = Sequential([
    # Conv block 1
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(2, 2),
    
    # Conv block 2
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    
    # Conv block 3
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    
    # Flatten and dense layers
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

# Train
history = model.fit(
    train_generator,
    epochs=20,
    validation_data=validation_generator,
    verbose=1
)

# Save model
model.save('anticheat_model_simple.h5')
```

#### Option B: MobileNetV2 (Better Accuracy)

```python
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout

# Load pre-trained MobileNetV2
base_model = MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)

# Freeze base model
base_model.trainable = False

# Add custom top layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train
history = model.fit(
    train_generator,
    epochs=10,
    validation_data=validation_generator
)

# Fine-tune: Unfreeze last layers
base_model.trainable = True
for layer in base_model.layers[:-20]:
    layer.trainable = False

model.compile(
    optimizer=Adam(learning_rate=0.00001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history_fine = model.fit(
    train_generator,
    epochs=10,
    validation_data=validation_generator
)

model.save('anticheat_model_mobilenet.h5')
```

### B∆∞·ªõc 4: Convert to TensorFlow.js

```bash
# Install tensorflowjs converter
pip install tensorflowjs

# Convert model
tensorflowjs_converter \
    --input_format keras \
    anticheat_model_mobilenet.h5 \
    tfjs_model/
```

### B∆∞·ªõc 5: Evaluate Model

```python
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Plot training history
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')

plt.tight_layout()
plt.savefig('training_history.png')

# Evaluate on test set
test_generator = datagen.flow_from_directory(
    'dataset/',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

predictions = model.predict(test_generator)
predicted_classes = (predictions > 0.5).astype(int)

# Confusion matrix
cm = confusion_matrix(test_generator.classes, predicted_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix.png')

# Classification report
print("\nClassification Report:")
print(classification_report(
    test_generator.classes,
    predicted_classes,
    target_names=['Normal', 'Suspicious']
))
```

## Method 3: Fine-tuning Pre-trained Models

### Using YOLO for Person Detection

```python
# YOLOv5 for detecting multiple people
import torch

# Load YOLOv5
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

def detect_people(image_path):
    results = model(image_path)
    
    # Filter for person class (class 0)
    people = results.pandas().xyxy[0]
    people = people[people['class'] == 0]
    
    return len(people), people

# Example usage
count, detections = detect_people('test_image.jpg')
print(f"Detected {count} people")
```

## Deployment v√† Integration

### Integrate v·ªõi Client

```typescript
// src/services/customAntiCheatService.ts
import * as tf from '@tensorflow/tfjs';

class CustomAntiCheatService {
  private model: tf.LayersModel | null = null;

  async loadModel() {
    this.model = await tf.loadLayersModel('/models/anticheat/model.json');
    console.log('Custom anti-cheat model loaded');
  }

  async predict(videoElement: HTMLVideoElement): Promise<number> {
    if (!this.model) {
      await this.loadModel();
    }

    // Capture frame
    const canvas = document.createElement('canvas');
    canvas.width = 224;
    canvas.height = 224;
    const ctx = canvas.getContext('2d')!;
    ctx.drawImage(videoElement, 0, 0, 224, 224);

    // Preprocess
    const tensor = tf.browser.fromPixels(canvas)
      .toFloat()
      .div(255.0)
      .expandDims(0);

    // Predict
    const prediction = this.model!.predict(tensor) as tf.Tensor;
    const score = await prediction.data();

    // Cleanup
    tensor.dispose();
    prediction.dispose();

    return score[0]; // 0-1, higher = more suspicious
  }

  async analyze(videoElement: HTMLVideoElement) {
    const suspiciousScore = await this.predict(videoElement);

    const warnings = [];

    if (suspiciousScore > 0.7) {
      warnings.push({
        type: 'suspicious_behavior',
        severity: 'high',
        details: `Suspicious behavior detected (confidence: ${(suspiciousScore * 100).toFixed(1)}%)`
      });
    } else if (suspiciousScore > 0.5) {
      warnings.push({
        type: 'suspicious_behavior',
        severity: 'medium',
        details: `Potentially suspicious behavior (confidence: ${(suspiciousScore * 100).toFixed(1)}%)`
      });
    }

    return warnings;
  }
}

export const customAntiCheatService = new CustomAntiCheatService();
```

## Best Practices

### 1. Dataset Quality

‚úÖ **Good practices**:
- Collect data from actual students
- Diverse lighting conditions
- Various camera angles
- Multiple people in dataset
- Balance classes (50/50 normal/suspicious)

‚ùå **Avoid**:
- Too little data (< 200 images per class)
- Imbalanced classes (90% normal, 10% suspicious)
- Only one person's face
- Same background for all images

### 2. Model Selection

| Model | Size | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| Simple CNN | ~5MB | Very Fast | 85% | Low-end devices |
| MobileNetV2 | ~15MB | Fast | 92% | **Recommended** |
| ResNet50 | ~100MB | Moderate | 95% | High accuracy needed |
| EfficientNet | ~30MB | Moderate | 94% | Balance |

### 3. Performance Optimization

```typescript
// Throttle predictions
let lastPrediction = 0;
const predictionInterval = 3000; // 3 seconds

async function monitorWithThrottle(videoElement: HTMLVideoElement) {
  const now = Date.now();
  
  if (now - lastPrediction >= predictionInterval) {
    const warnings = await customAntiCheatService.analyze(videoElement);
    lastPrediction = now;
    return warnings;
  }
  
  return [];
}
```

### 4. False Positive Handling

```typescript
// Require multiple consecutive detections
class WarningBuffer {
  private buffer: any[] = [];
  private bufferSize = 3;

  addWarning(warning: any) {
    this.buffer.push(warning);
    if (this.buffer.length > this.bufferSize) {
      this.buffer.shift();
    }
  }

  shouldReport(): boolean {
    // Only report if warning appears in majority of recent checks
    return this.buffer.length >= this.bufferSize &&
           this.buffer.filter(w => w.type === 'suspicious_behavior').length >= 2;
  }
}
```

## Troubleshooting

### Model qu√° ch·∫≠m

**Solution**: Reduce model size, lower resolution, increase prediction interval

```typescript
// Lower resolution
const tensor = tf.browser.fromPixels(canvas)
  .resizeBilinear([128, 128])  // Instead of 224x224
  .toFloat()
  .div(255.0)
  .expandDims(0);
```

### Nhi·ªÅu false positives

**Solution**: Adjust threshold, add buffer, collect more training data

```typescript
// Higher threshold
if (suspiciousScore > 0.8) {  // Instead of 0.7
  // Report warning
}
```

### Model kh√¥ng accurate

**Solution**: More training data, better augmentation, different architecture

## Resources

- [TensorFlow.js Models](https://www.tensorflow.org/js/models)
- [BlazeFace](https://github.com/tensorflow/tfjs-models/tree/master/blazeface)
- [YOLOv5](https://github.com/ultralytics/yolov5)
- [OpenCV Python Tutorial](https://docs.opencv.org/master/d6/d00/tutorial_py_root.html)

## Summary

**Default (No Training)**:
- ‚úÖ Use BlazeFace for face detection
- ‚úÖ Works immediately
- ‚úÖ 95% accuracy for basic cases

**Custom Model (Advanced)**:
- üìä Collect 500+ images per class
- ü§ñ Train with MobileNetV2
- üöÄ Convert to TensorFlow.js
- üì± Deploy to client

**Remember**: H·ªá th·ªëng m·∫∑c ƒë·ªãnh v·ªõi BlazeFace ƒë·ªß t·ªët cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p. Ch·ªâ training custom model khi c√≥ y√™u c·∫ßu ƒë·∫∑c bi·ªát!
